---
title: "Who Asks the Questions?"
date: "2026-01-26"
---
I've been using these tools daily for more than a year now for a variety of tasks. Over the course of the year ([the year of Claude Code])(./2025-12-28-notes-on-karpathy-year-review.md) they improved dramatically. Roughly put in the beginning of 2025 (gpt 3?) they were a gimmick and at the end of 2025 they are a ubiquitous in various workflows. The most improvement is obviously in coding, Claude Code and other tools now write the code for most of the people I know, including myself of course. But one thing that they haven't improved much is in writing about the code. I'd like to unpack why and what that means for extrapolating forward. 

It seems to me that the current AI models have limited situational awareness, and I don't think this problem can be solved by scale. In fact it might be worse with scale. They can handle a vast volume of information but treat it all with equal gravity. Humans, can handle a lot less info at once, and rely on complex workflows to keep abreast of large volumes of information (big data technologies being one) but compensate by having good heuristics about what is important *in the context they are in*. 

For example, if you ask an AI to explain a code repo, it will give you a description of the modules and how they are connected, but it won't be able to tell you what that does really. Of course the right answer is deeply contextual, so it's not a fair question to simply ask to explain it. It depends who is asking. If you are a user of the module you need an explanation of what you can do with the repo, if you are a developer you need to know how the modules are structured in relation to the goal, and so on. In fact even within those broad categories there is more context specific types, if you are a power user you need a different explanation than if you are not familiar with the tool etc. I have found that the AI struggles to understand that and give appropriate documentation. It is true that if you carefully curate the context in the prompt the quality of the answer will improve. But the effort needed to do this is disproportional with the gains. And even then the answer is subpar, relative to the actual code which usually comes out good with minimal prompting. 

Another area where this limitation shows up is with AI for mathematics. The progress is spectacular, we are able to train AI models that perform at the gold medal level in math olympiads, which is truly astonishing. There are now multiple startups that build math copilots that are able to prove theorems and do fundamental mathematical research autonomously. But, while AIs can prove many theorems, they are not sure which theorems to prove. Math is full of statements that are true but useless. The point is not to find all true statements, but to find the right succession of true statements that advance our knowledge of the field. A similar issue is being observed in life science where AIs have made amazing progress but still haven't cracked the research piece. This is the same issue that Demis Hassabis mentioned in his Davos [chat](https://www.youtube.com/watch?v=mmKAnHz36v0) with Dario Amodei 

>Some areas of natural science are much harder to do than that. You won't necessarily know if the chemical compound you've built or this prediction about physics is correct. It may be h you may have to test it experimentally and that will all take longer. So uh I also think there are some missing capabilities at the moment. uh in terms of like not just solving existing conjectures uh or existing problems but actually coming up with the question in the first place or coming up with the theory or the hypothesis. I think that's much much harder And I think that's the highest level of scientific creativity. And it's not clear I think we will have those systems, so I don't think it's impossible, but I think there may be one or two missing ingredients. 


Why is it that AI's can't ask the right questions? I think the simple answer is "context engineering". In fact engineers realized that context is the critical  ingredient that has to be curated for the AI for it to work. And that is kind of curious if you think about it because it reverses the roles of the AI and humans. AI curates so much content for us, it reads whole repos, articles emails and comes back with a summary which would take us a long time to do. But when it comes to context, what matters versus not, is *us* who have to curate it for the AI. 

If we go one level deeper, I think the AI has no hope of understanding the context of course. It's just a next token predictor which if we think of it is a giant bell curve of all the internet text. But while AI serves the average of the curve, context is usually in the "variance". Context is how the same information means different things to different people. The current approach to solve this is issue is reasoning, which really means spend more tokens before answering. But just scaling up seems to me to make things only worse. If you need more  information to figure what is important in the first place, you only increased the total volume of information you have to process. To avoid being overwhelmed, you need a way to distill information down to what is important before bringing in more information, and that is the step that AI seems to be struggling with. 

So what does this mean for future of these models? AI is changing rapidly at this stage which makes predictions hard. If we extrapolated into the future a year ago we could perhaps see that coding was getting better and better but I wouldn't have predicted that we could get gold medal performance in math olympiad within the year. Remember math olympiad problems are made each year from scratch so the machines have no hope of having seen the problems before. But I would have expected the AIs to become good at explaining code. I think we are starting to see the strengths and weaknesses of these systems. I am not confident that this situational awareness issue can be solved with scaling alone. I think we will need another breakthrough, of the size of test time compute to solve it (maybe *memory* and scaling together can solve it?). Until then I expect more success in verifiable tasks. So AIs will get better and better at answering, but humans will continue being the ones who ask the questions.

